{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PreprocessData.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPYtx+WZrVh0nRAzeYc+rEU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install \"tensorflow-text==2.8.*\""],"metadata":{"id":"r44XyqHJE603"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LTzKo7llUofp"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import re\n","import itertools\n","import tqdm\n","import pickle\n","import random\n","import torch\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import tensorflow as tf\n","import tensorflow_text as tf_text\n","import unicodedata"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pjd5RVtpY5kb","executionInfo":{"status":"ok","timestamp":1655460125786,"user_tz":-420,"elapsed":3586,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"00688950422503571962"}},"outputId":"2ad22455-a61c-4d34-d0c9-291482e60a01"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["**data**"],"metadata":{"id":"cIRJ-biHZBlp"}},{"cell_type":"code","source":["def check_valid_sentence():\n","    intab_l = \"ạảãàáâậầấẩẫăắằặẳẵóòọõỏôộổỗồốơờớợởỡéèẻẹẽêếềệểễúùụủũưựữửừứíìịỉĩýỳỷỵỹđ\"\n","    ascii_lowercase = 'abcdefghijklmnopqrstuvwxyz'\n","    digits = '0123456789'\n","    punctuation = r\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"\n","    whitespace = ' '\n","\n","    accept_strings =  intab_l + ascii_lowercase + digits + punctuation + whitespace\n","    r = re.compile('^[' + accept_strings + ']+$')\n","\n","    return r"],"metadata":{"id":"GOa61fOm1rGh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_path = \"/content/drive/MyDrive/NLP_Task/CorrectSpellingTask/train_tieng_viet.txt\"\n","\n","with open(data_path) as f:\n","    train = f.readlines()"],"metadata":{"id":"GoATN5uZZDkP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**eliminate sign for dataset**"],"metadata":{"id":"ObGZR71l2ceg"}},{"cell_type":"code","source":["intab_l = \"ạảãàáâậầấẩẫăắằặẳẵóòọõỏôộổỗồốơờớợởỡéèẻẹẽêếềệểễúùụủũưựữửừứíìịỉĩýỳỷỵỹđ\"\n","intab_u = \"ẠẢÃÀÁÂẬẦẤẨẪĂẮẰẶẲẴÓÒỌÕỎÔỘỔỖỒỐƠỜỚỢỞỠÉÈẺẸẼÊẾỀỆỂỄÚÙỤỦŨƯỰỮỬỪỨÍÌỊỈĨÝỲỶỴỸĐ\"\n","intab = list(intab_l+intab_u)\n","\n","outtab_l = \"a\"*17 + \"o\"*17 + \"e\"*11 + \"u\"*11 + \"i\"*5 + \"y\"*5 + \"d\"\n","outtab_u = \"A\"*17 + \"O\"*17 + \"E\"*11 + \"U\"*11 + \"I\"*5 + \"Y\"*5 + \"D\"\n","outtab = list(outtab_l + outtab_u)\n","\n","r = re.compile(\"|\".join(intab))\n","replaces_dict = dict(zip(intab, outtab))"],"metadata":{"id":"TsBqqRwA2R-Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#bỏ dấu câu\n","def remove_sign(sen):\n","    sen_new = r.sub(lambda m: replaces_dict[m.group(0)], sen)\n","    return sen_new\n","\n","#tách dấu phẩy, chấm\n","def normalizeString(s):\n","    # Tách dấu câu nếu kí tự liền nhau\n","    marks = '[.!?,-${}()]'\n","    r = \"([\"+\"\\\\\".join(marks)+\"])\"\n","    s = re.sub(r, r\" \\1 \", s)\n","    # Thay thế nhiều spaces bằng 1 space.\n","    s = re.sub(r\"\\s+\", r\" \", s).strip()\n","    return s"],"metadata":{"id":"1PnWJpG83vRm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Split Dataset**"],"metadata":{"id":"r55p_clyiupJ"}},{"cell_type":"code","source":["train_idx_180k = []\n","train_opt_180k = []\n","train_ipt_180k = []\n","val_idx_10k = []\n","val_opt_10k = []\n","val_ipt_10k = []\n","test_idx_10k = []\n","test_opt_10k = []\n","test_ipt_10k = []\n","\n","for i in range(200000):\n","    [idx, sen] = train[i].split('\\t')\n","    sen = sen[:-1]\n","    try:\n","        non_sign_sen = normalizeString(remove_sign(sen))\n","    except:\n","        continue\n","    sen = normalizeString(sen)\n","    if i < 180000:\n","        train_idx_180k.append(idx)\n","        train_opt_180k.append(sen)\n","        train_ipt_180k.append(non_sign_sen)\n","    elif i < 190000:\n","        val_idx_10k.append(idx)\n","        val_opt_10k.append(sen)\n","        val_ipt_10k.append(non_sign_sen)\n","    elif i < 200000:\n","        test_idx_10k.append(idx)\n","        test_opt_10k.append(sen)\n","        test_ipt_10k.append(non_sign_sen)"],"metadata":{"id":"Lr6OrkNHlEjI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _save_pickle(filename, obj):\n","  with open(filename, 'wb') as f:\n","    pickle.dump(obj, f)\n","\n","_save_pickle('train_tv_idx_180k.pkl', train_idx_180k)\n","_save_pickle('val_tv_idx_10k.pkl', val_idx_10k)\n","_save_pickle('test_tv_idx_10k.pkl', test_idx_10k)"],"metadata":{"id":"UjtW-DzNmJDX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Normalized Data**"],"metadata":{"id":"FIS72Dkztl7a"}},{"cell_type":"code","source":["def _ngram(text, length = 4):\n","    words = text.split()\n","    grams = []\n","    if len(words) <= length:\n","      words = words + [\"PAD\"]*(length-len(words))\n","      return [' '.join(words)]\n","    else:\n","      for i in range(len(words)-length+1):\n","        grams.append(' '.join(words[i:(i+length)]))\n","      return grams\n","\n","train_grams = list(itertools.chain.from_iterable([_ngram(item) for item in train_opt_180k]))\n","train_rev_acc_grams = list(itertools.chain.from_iterable([_ngram(item) for item in train_ipt_180k]))\n","\n","val_grams = list(itertools.chain.from_iterable([_ngram(item) for item in val_opt_10k]))\n","val_rev_acc_grams = list(itertools.chain.from_iterable([_ngram(item) for item in val_ipt_10k]))\n","\n","test_grams = list(itertools.chain.from_iterable([_ngram(item) for item in test_opt_10k]))\n","test_rev_acc_grams = list(itertools.chain.from_iterable([_ngram(item) for item in test_ipt_10k]))\n","\n","corpus_train = list(zip(train_rev_acc_grams, train_grams))\n","corpus_val = list(zip(val_rev_acc_grams, val_grams))\n","corpus_test = list(zip(test_rev_acc_grams, test_grams))"],"metadata":{"id":"-9JDDXFltnzF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Create voc cabulary**"],"metadata":{"id":"rEmJ99Kxzkfb"}},{"cell_type":"code","source":["def _load_pickle(filename):\n","    pickle_in = open(filename,\"rb\")\n","    dict_ = pickle.load(pickle_in)\n","\n","    return dict_\n","\n","def _save_pickle(filename, obj):\n","  with open(filename, 'wb') as f:\n","    pickle.dump(obj, f)"],"metadata":{"id":"gLczvOf-0Mwz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_save = \"/content/drive/MyDrive/NLP_Task/CorrectSpellingTask/\"\n","\n","_save_pickle(data_save + 'corpus_train.pkl', corpus_train)\n","_save_pickle(data_save + 'corpus_val.pkl', corpus_val)\n","_save_pickle(data_save + 'corpus_test.pkl', corpus_test)"],"metadata":{"id":"WehIh7CTyFJ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_save = \"/content/drive/MyDrive/NLP_Task/CorrectSpellingTask/\"\n"," \n","corpus_train = _load_pickle(data_save + 'corpus_train.pkl')\n","corpus_val = _load_pickle(data_save + 'corpus_val.pkl')\n","corpus_test = _load_pickle(data_save + 'corpus_test.pkl')"],"metadata":{"id":"X8HAf6Kk0ueu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inp = [ele[0] for ele in corpus_train[:100000]]\n","tar = [ele[1] for ele in corpus_train[:100000]] "],"metadata":{"id":"J0DKtTo0FcvI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BUFFER_SIZE = len(corpus_train)\n","BATCH_SIZE = 64\n","\n","dataset = tf.data.Dataset.from_tensor_slices((inp, tar)).shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE)"],"metadata":{"id":"_PFsmD5GFHI7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["PAD_token = 0  # Used for padding short sentences\n","SOS_token = 1  # Start-of-sentence token\n","EOS_token = 2  # End-of-sentence token\n","\n","def preprocess(text):\n","    text = tf_text.normalize_utf8(text, 'NFKD')\n","    text = tf.strings.lower(text)\n","    print(text)\n","    text = unicodeToAscii(text)\n","    text = tf.strings.join(['[SOS]', text, '[EOS]'], separator=' ')\n","  \n","    return text"],"metadata":{"id":"uylumyYPIK2Z"},"execution_count":null,"outputs":[]}]}