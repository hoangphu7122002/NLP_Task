{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"accent_model_Transformer.ipynb","provenance":[],"authorship_tag":"ABX9TyOmcFS0QPGFDonxKa3k9qsd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"O1RYW1nOaB0u"},"outputs":[],"source":["!pip install \"tensorflow-text==2.8.*\""]},{"cell_type":"code","source":["!pip install tensorflow_datasets"],"metadata":{"id":"nNBUmhkF9nSI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import re\n","import itertools\n","import tqdm\n","import pickle\n","import random\n","import torch\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import tensorflow as tf\n","import tensorflow_text as tf_text\n","import unicodedata\n","import tensorflow_datasets\n","import typing\n","from typing import Any, Tuple\n","from tqdm import tqdm\n","import tensorflow_datasets as tfds\n","import time"],"metadata":{"id":"mg6er-p29ohe","executionInfo":{"status":"ok","timestamp":1655831364560,"user_tz":-420,"elapsed":448,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ytUYCUK89rm7","executionInfo":{"status":"ok","timestamp":1655826797099,"user_tz":-420,"elapsed":25998,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}},"outputId":"38a3d027-aa0f-47e5-c0ea-e6843029d4d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["**dataset**"],"metadata":{"id":"JAsUgSfU-USy"}},{"cell_type":"code","source":["def _load_pickle(filename):\n","    pickle_in = open(filename,\"rb\")\n","    dict_ = pickle.load(pickle_in)\n","\n","    return dict_\n","\n","def _save_pickle(filename, obj):\n","  with open(filename, 'wb') as f:\n","    pickle.dump(obj, f)"],"metadata":{"id":"nhCNQ5fd-VQi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_save = \"/content/drive/MyDrive/NLP_Task/CorrectSpellingTask/\"\n","\n","sentences = _load_pickle(data_save + 'sentences.pkl')"],"metadata":{"id":"RLUxtzFi-hCi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["accented_chars = {\n","    'a': u'a á à ả ã ạ â ấ ầ ẩ ẫ ậ ă ắ ằ ẳ ẵ ặ',\n","    'o': u'o ó ò ỏ õ ọ ô ố ồ ổ ỗ ộ ơ ớ ờ ở ỡ ợ',\n","    'e': u'e é è ẻ ẽ ẹ ê ế ề ể ễ ệ',\n","    'u': u'u ú ù ủ ũ ụ ư ứ ừ ử ữ ự',\n","    'i': u'i í ì ỉ ĩ ị',\n","    'y': u'y ý ỳ ỷ ỹ ỵ',\n","    'd': u'd đ',\n","}\n","\n","plain_char_map = {}\n","for c, variants in accented_chars.items():\n","    for v in variants.split(' '):\n","        plain_char_map[v] = c\n","\n","\n","def remove_accent(text):\n","    return u''.join(plain_char_map.get(char, char) for char in text)"],"metadata":{"id":"dH-0lcrQ-1DG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_opt_160k = []\n","train_ipt_160k = []\n","\n","val_opt_20k = []\n","val_ipt_20k = []\n","\n","test_opt_20k = []\n","test_ipt_20k = []\n","\n","for i in tqdm(range(len(sentences))):\n","  try:\n","    non_acc_seq = remove_accent(sentences[i])\n","  except:\n","    print('error remove tone line at sequence {}', str(i))\n","    next\n","  if i < 160000:\n","    train_opt_160k.append(sentences[i])\n","    train_ipt_160k.append(non_acc_seq)\n","  elif i < 180000:\n","    val_opt_20k.append(sentences[i])\n","    val_ipt_20k.append(non_acc_seq)\n","  else:\n","    test_opt_20k.append(sentences[i])\n","    test_ipt_20k.append(non_acc_seq)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sDabMkOP-1kf","executionInfo":{"status":"ok","timestamp":1655827342990,"user_tz":-420,"elapsed":10914,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}},"outputId":"e3d214ec-9dc8-4625-f7d2-773d6cb3800a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 200000/200000 [00:10<00:00, 18884.21it/s]\n"]}]},{"cell_type":"code","source":["train_examples = tf.data.Dataset.from_tensor_slices((train_ipt_160k, train_opt_160k))\n","val_examples = tf.data.Dataset.from_tensor_slices((val_ipt_20k, val_opt_20k))\n","test_examples = tf.data.Dataset.from_tensor_slices((test_ipt_20k, test_opt_20k))"],"metadata":{"id":"fpGJislzAfj8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**build tokenizer -> to convert text to vector**"],"metadata":{"id":"BRhRa57yA1hI"}},{"cell_type":"code","source":["tokenizer_ipt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n","    (ipt.numpy() for (ipt, opt) in train_examples), target_vocab_size=2**13)\n","\n","tokenizer_opt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n","    (opt.numpy() for (ipt, opt) in train_examples), target_vocab_size=2**13)"],"metadata":{"id":"z7sYaSnYBZYO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_save_pickle(data_save + 'tokenizer_ipt.pkl', tokenizer_ipt)\n","_save_pickle(data_save + 'tokenizer_opt.pkl', tokenizer_opt)"],"metadata":{"id":"-z8BUm76CIz-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BUFFER_SIZE = 20000\n","BATCH_SIZE = 64\n","MAX_LENGTH = 40\n","\n","def encode(ipt, opt):\n","  ipt = [tokenizer_ipt.vocab_size] + tokenizer_ipt.encode(\n","      ipt.numpy()) + [tokenizer_ipt.vocab_size+1]\n","\n","  opt = [tokenizer_opt.vocab_size] + tokenizer_opt.encode(\n","      opt.numpy()) + [tokenizer_opt.vocab_size+1]  \n","  return ipt, opt\n","\n","def tf_encode(ipt, opt):\n","  result_ipt, result_opt = tf.py_function(encode, [ipt, opt], [tf.int64, tf.int64])\n","  result_ipt.set_shape([None])\n","  result_opt.set_shape([None])\n","  return result_ipt, result_opt\n","\n","\n","def filter_max_length(x, y, max_length=MAX_LENGTH):\n","  return tf.logical_and(tf.size(x) <= max_length,\n","                        tf.size(y) <= max_length)"],"metadata":{"id":"qBlFGtn3DZHO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = train_examples.map(tf_encode)\n","train_dataset = train_dataset.filter(filter_max_length)\n","# cache the dataset to memory to get a speedup while reading from it.\n","train_dataset = train_dataset.cache()\n","train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n","train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n","\n","val_dataset = val_examples.map(tf_encode)\n","val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)\n","\n","test_dataset = test_examples.map(tf_encode)\n","test_dataset = test_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)"],"metadata":{"id":"HapjCCsGDxz8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Position encoding**"],"metadata":{"id":"Dvwj7TD7ERV_"}},{"cell_type":"code","source":["def get_angles(pos, i, d_model):\n","  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n","  return pos * angle_rates\n","\n","def positional_encoding(position, d_model):\n","  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n","                          np.arange(d_model)[np.newaxis, :],\n","                          d_model) # shape (position, d_model)\n","  \n","  # apply sin to even indices in the array; 2i\n","  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","  \n","  # apply cos to odd indices in the array; 2i+1\n","  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","    \n","  pos_encoding = angle_rads[np.newaxis, ...]\n","    \n","  return tf.cast(pos_encoding, dtype=tf.float32) # shape: (position, d_model)"],"metadata":{"id":"n2jzM99VES6K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Masking**"],"metadata":{"id":"NuHovpXrGiYa"}},{"cell_type":"code","source":["#for don't predict padding word\n","def create_padding_mask(seq):\n","  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","  \n","  # add extra dimensions to add the padding\n","  # to the attention logits.\n","  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"],"metadata":{"id":"A2ili4MdGTwj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#for don't permit word in the future affect word in the past\n","def create_look_ahead_mask(size):\n","  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","  return mask  # (seq_len, seq_len)"],"metadata":{"id":"ZBLWW6XAG3Gc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**scale dot production**"],"metadata":{"id":"Z5u6fZyCHO9l"}},{"cell_type":"code","source":["def scaled_dot_product_attention(q, k, v, mask):\n","  \"\"\"Calculate the attention weights.\n","  q, k, v must have matching leading dimensions.\n","  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n","  The mask has different shapes depending on its type(padding or look ahead) \n","  but it must be broadcastable for addition.\n","  \n","  Args:\n","    q: query shape == (..., seq_len_q, depth)\n","    k: key shape == (..., seq_len_k, depth)\n","    v: value shape == (..., seq_len_v, depth_v)\n","    mask: Float tensor with shape broadcastable \n","          to (..., seq_len_q, seq_len_k). Defaults to None.\n","    \n","  Returns:\n","    output, attention_weights\n","  \"\"\"\n","\n","  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n","  \n","  # scale matmul_qk\n","  dk = tf.cast(tf.shape(k)[-1], tf.float32) # depth\n","  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","  # add the mask to the scaled tensor.\n","  if mask is not None:\n","    scaled_attention_logits += (mask * -1e9)  \n","\n","  # softmax is normalized on the last axis (seq_len_k) so that the scores\n","  # add up to 1.\n","  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n","\n","  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n","\n","  return output, attention_weights"],"metadata":{"id":"vaDF50gYHQSB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Multi-head attention**"],"metadata":{"id":"iOzLk49MIZw1"}},{"cell_type":"code","source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","  def __init__(self, d_model, num_heads):\n","    super(MultiHeadAttention, self).__init__()\n","    self.num_heads = num_heads\n","    self.d_model = d_model\n","    \n","    assert d_model % self.num_heads == 0\n","    \n","    self.depth = d_model // self.num_heads\n","    \n","    self.wq = tf.keras.layers.Dense(d_model)\n","    self.wk = tf.keras.layers.Dense(d_model)\n","    self.wv = tf.keras.layers.Dense(d_model)\n","    \n","    self.dense = tf.keras.layers.Dense(d_model)\n","        \n","  def split_heads(self, x, batch_size):\n","    \"\"\"Split the last dimension into (num_heads, depth).\n","    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n","    \"\"\"\n","    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","    return tf.transpose(x, perm=[0, 2, 1, 3])\n","    \n","  def call(self, v, k, q, mask):\n","    batch_size = tf.shape(q)[0]\n","    \n","    q = self.wq(q)  # (batch_size, seq_len, d_model)\n","    k = self.wk(k)  # (batch_size, seq_len, d_model)\n","    v = self.wv(v)  # (batch_size, seq_len, d_model)\n","    \n","    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n","    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n","    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n","    \n","    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n","    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n","    scaled_attention, attention_weights = scaled_dot_product_attention(\n","        q, k, v, mask)\n","    \n","    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n","\n","    concat_attention = tf.reshape(scaled_attention, \n","                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n","\n","    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n","        \n","    return output, attention_weights"],"metadata":{"id":"H3pfduG8IbbT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Point-wise feed forward**"],"metadata":{"id":"KIPkfdoBI491"}},{"cell_type":"code","source":["def point_wise_feed_forward_network(d_model, dff):\n","  return tf.keras.Sequential([\n","      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n","      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n","  ])"],"metadata":{"id":"l7q4lByZI7YJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Encoder Layer**"],"metadata":{"id":"64OrM4KgJF4f"}},{"cell_type":"code","source":["class EncoderLayer(tf.keras.layers.Layer):\n","  def __init__(self, d_model, num_heads, dff, rate=0.1):\n","    super(EncoderLayer, self).__init__()\n","\n","    self.mha = MultiHeadAttention(d_model, num_heads)\n","    self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    \n","    self.dropout1 = tf.keras.layers.Dropout(rate)\n","    self.dropout2 = tf.keras.layers.Dropout(rate)\n","    \n","  def call(self, x, training, mask):\n","\n","    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n","    attn_output = self.dropout1(attn_output, training=training)\n","    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n","    \n","    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n","    ffn_output = self.dropout2(ffn_output, training=training)\n","    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n","    \n","    return out2"],"metadata":{"id":"4oP294oTLsUl","executionInfo":{"status":"ok","timestamp":1655830406017,"user_tz":-420,"elapsed":406,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["**DecoderLayer**"],"metadata":{"id":"28SM-INGL9fh"}},{"cell_type":"code","source":["class DecoderLayer(tf.keras.layers.Layer):\n","  def __init__(self, d_model, num_heads, dff, rate=0.1):\n","    super(DecoderLayer, self).__init__()\n","\n","    self.mha1 = MultiHeadAttention(d_model, num_heads)\n","    self.mha2 = MultiHeadAttention(d_model, num_heads)\n","\n","    self.ffn = point_wise_feed_forward_network(d_model, dff)\n"," \n","    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    \n","    self.dropout1 = tf.keras.layers.Dropout(rate)\n","    self.dropout2 = tf.keras.layers.Dropout(rate)\n","    self.dropout3 = tf.keras.layers.Dropout(rate)\n","    \n","    \n","  def call(self, x, enc_output, training, \n","           look_ahead_mask, padding_mask):\n","    # enc_output.shape == (batch_size, input_seq_len, d_model)\n","\n","    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n","    attn1 = self.dropout1(attn1, training=training)\n","    out1 = self.layernorm1(attn1 + x)\n","    \n","    attn2, attn_weights_block2 = self.mha2(\n","        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n","    attn2 = self.dropout2(attn2, training=training)\n","    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n","    \n","    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n","    ffn_output = self.dropout3(ffn_output, training=training)\n","    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n","    \n","    return out3, attn_weights_block1, attn_weights_block2"],"metadata":{"id":"HqceMiOgMGwU","executionInfo":{"status":"ok","timestamp":1655830669807,"user_tz":-420,"elapsed":346,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["**Encoder**"],"metadata":{"id":"fGXvbb8JM0qo"}},{"cell_type":"code","source":["class Encoder(tf.keras.layers.Layer):\n","  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n","               maximum_position_encoding, rate=0.1):\n","    super(Encoder, self).__init__()\n","\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","    \n","    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n","    self.pos_encoding = positional_encoding(maximum_position_encoding, \n","                                            self.d_model)\n","    \n","    \n","    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n","                       for _ in range(num_layers)]\n","  \n","    self.dropout = tf.keras.layers.Dropout(rate)\n","        \n","  def call(self, x, training, mask):\n","\n","    seq_len = tf.shape(x)[1]\n","    \n","    # adding embedding and position encoding.\n","    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n","    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","    x += self.pos_encoding[:, :seq_len, :]\n","\n","    x = self.dropout(x, training=training)\n","    \n","    for i in range(self.num_layers):\n","      x = self.enc_layers[i](x, training, mask)\n","    \n","    return x  # (batch_size, input_seq_len, d_model)"],"metadata":{"id":"PJa_uV0xM1oc","executionInfo":{"status":"ok","timestamp":1655830713331,"user_tz":-420,"elapsed":402,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":["**Decoder**"],"metadata":{"id":"bVp54EfEM136"}},{"cell_type":"code","source":["class Decoder(tf.keras.layers.Layer):\n","  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n","               maximum_position_encoding, rate=0.1):\n","    super(Decoder, self).__init__()\n","\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","    \n","    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n","    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n","    \n","    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n","                       for _ in range(num_layers)]\n","    self.dropout = tf.keras.layers.Dropout(rate)\n","    \n","  def call(self, x, enc_output, training, \n","           look_ahead_mask, padding_mask):\n","\n","    seq_len = tf.shape(x)[1]\n","    attention_weights = {}\n","    \n","    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n","    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","    x += self.pos_encoding[:, :seq_len, :]\n","    \n","    x = self.dropout(x, training=training)\n","\n","    for i in range(self.num_layers):\n","      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n","                                             look_ahead_mask, padding_mask)\n","      \n","      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n","      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n","    \n","    # x.shape == (batch_size, target_seq_len, d_model)\n","    return x, attention_weights"],"metadata":{"id":"R4OMJaO6M59O","executionInfo":{"status":"ok","timestamp":1655830755835,"user_tz":-420,"elapsed":417,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["**Transformer**"],"metadata":{"id":"t0c_tGrPNIeK"}},{"cell_type":"code","source":["class Transformer(tf.keras.Model):\n","  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n","               target_vocab_size, pe_input, pe_target, rate=0.1):\n","    super(Transformer, self).__init__()\n","\n","    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n","                           input_vocab_size, pe_input, rate)\n","\n","    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n","                           target_vocab_size, pe_target, rate)\n","\n","    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","    \n","  def call(self, inp, tar, training, enc_padding_mask, \n","           look_ahead_mask, dec_padding_mask):\n","    # print('enc_padding_mask: ', enc_padding_mask)\n","    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n","    \n","    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n","    dec_output, attention_weights = self.decoder(\n","        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n","    \n","    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n","    \n","    return final_output, attention_weights"],"metadata":{"id":"4XcnCDX3NJkJ","executionInfo":{"status":"ok","timestamp":1655830787983,"user_tz":-420,"elapsed":513,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":["**HyperParameter**"],"metadata":{"id":"vNpKy7i2NQ3y"}},{"cell_type":"code","source":["num_layers = 4\n","d_model = 128\n","dff = 512\n","num_heads = 8\n","\n","input_vocab_size = tokenizer_ipt.vocab_size + 2\n","target_vocab_size = tokenizer_opt.vocab_size + 2\n","dropout_rate = 0.1"],"metadata":{"id":"CBlFewW5NSwJ","executionInfo":{"status":"ok","timestamp":1655830834178,"user_tz":-420,"elapsed":10,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","  def __init__(self, d_model, warmup_steps=4000):\n","    super(CustomSchedule, self).__init__()\n","    \n","    self.d_model = d_model\n","    self.d_model = tf.cast(self.d_model, tf.float32)\n","\n","    self.warmup_steps = warmup_steps\n","    \n","  def __call__(self, step):\n","    arg1 = tf.math.rsqrt(step)\n","    arg2 = step * (self.warmup_steps ** -1.5)\n","    \n","    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"],"metadata":{"id":"mh1As9tDNfSC","executionInfo":{"status":"ok","timestamp":1655830881115,"user_tz":-420,"elapsed":421,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["learning_rate = CustomSchedule(d_model)\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n","                                     epsilon=1e-9)"],"metadata":{"id":"_76fv0KKNiYW","executionInfo":{"status":"ok","timestamp":1655830891324,"user_tz":-420,"elapsed":319,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":["**Loss and Metrics**"],"metadata":{"id":"qr2Wg4KONzQe"}},{"cell_type":"code","source":["loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"],"metadata":{"id":"IgKEzeDnN0or","executionInfo":{"status":"ok","timestamp":1655830960484,"user_tz":-420,"elapsed":5,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["def loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = loss_object(real, pred)\n","\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","  \n","  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"],"metadata":{"id":"QUyAYuUJN2jX","executionInfo":{"status":"ok","timestamp":1655830969703,"user_tz":-420,"elapsed":653,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n","    name='train_accuracy')"],"metadata":{"id":"-AiQZcjYN7mp","executionInfo":{"status":"ok","timestamp":1655830987989,"user_tz":-420,"elapsed":3,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":["**Training**"],"metadata":{"id":"jBzQ1VjzN_QZ"}},{"cell_type":"code","source":["transformer = Transformer(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff,\n","                          input_vocab_size=input_vocab_size, target_vocab_size=target_vocab_size, \n","                          pe_input=input_vocab_size, \n","                          pe_target=target_vocab_size,\n","                          rate=dropout_rate)"],"metadata":{"id":"Fm345975N-eX","executionInfo":{"status":"ok","timestamp":1655831018904,"user_tz":-420,"elapsed":305,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["def create_masks(inp, tar):\n","  # Encoder padding mask\n","  enc_padding_mask = create_padding_mask(inp)\n","\n","  # Used in the 2nd attention block in the decoder.\n","  # This padding mask is used to mask the encoder outputs.\n","  dec_padding_mask = create_padding_mask(inp)\n","  \n","  # Used in the 1st attention block in the decoder.\n","  # It is used to pad and mask future tokens in the input received by \n","  # the decoder.\n","  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n","  dec_target_padding_mask = create_padding_mask(tar)\n","  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n","  return enc_padding_mask, combined_mask, dec_padding_mask"],"metadata":{"id":"OO1jvwuNOEWq","executionInfo":{"status":"ok","timestamp":1655831079946,"user_tz":-420,"elapsed":405,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["checkpoint_path = data_save + \"/checkpoints/train_500k\"\n","\n","ckpt = tf.train.Checkpoint(transformer=transformer,\n","                           optimizer=optimizer)\n","\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","\n","# if a checkpoint exists, restore the latest checkpoint.\n","if ckpt_manager.latest_checkpoint:\n","  ckpt.restore(ckpt_manager.latest_checkpoint)\n","  print ('Latest checkpoint restored!!')"],"metadata":{"id":"hnkvP3JJOEaF","executionInfo":{"status":"ok","timestamp":1655831146721,"user_tz":-420,"elapsed":297,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":["**Combine model**"],"metadata":{"id":"x8sM4GmaPI9G"}},{"cell_type":"code","source":["EPOCHS = 20\n","train_step_signature = [\n","    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n","    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n","]\n","\n","@tf.function(input_signature=train_step_signature)\n","def train_step(inp, tar):\n","  tar_inp = tar[:, :-1]\n","  tar_real = tar[:, 1:]\n","  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n","  with tf.GradientTape() as tape:\n","    predictions, _ = transformer(inp, tar_inp, \n","                                 True, \n","                                 enc_padding_mask, \n","                                 combined_mask, \n","                                 dec_padding_mask)\n","    \n","    loss = loss_function(tar_real, predictions)\n","\n","  gradients = tape.gradient(loss, transformer.trainable_variables)    \n","  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n","  \n","  train_loss(loss)\n","  train_accuracy(tar_real, predictions)"],"metadata":{"id":"_X0I5dRBPK9c","executionInfo":{"status":"ok","timestamp":1655831330013,"user_tz":-420,"elapsed":362,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["for epoch in range(EPOCHS):\n","  start = time.time()\n","  \n","  train_loss.reset_states()\n","  train_accuracy.reset_states()\n","  \n","  # inp -> non_diacritic, tar -> diacritic\n","  for (batch, (inp, tar)) in enumerate(train_dataset):\n","    train_step(inp, tar)\n","    \n","    if batch % 50 == 0:\n","      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n","          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n","      \n","  if (epoch + 1) % 5 == 0:\n","    ckpt_save_path = ckpt_manager.save()\n","    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n","                                                         ckpt_save_path))\n","    \n","  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n","                                                train_loss.result(), \n","                                                train_accuracy.result()))\n","\n","  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KdFKL6DzPQFZ","executionInfo":{"status":"ok","timestamp":1655832555529,"user_tz":-420,"elapsed":1186407,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}},"outputId":"70518079-416c-4731-cd5e-a65e74d5d664"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1 Batch 0 Loss 9.0268 Accuracy 0.0004\n","Epoch 1 Batch 50 Loss 8.9782 Accuracy 0.0049\n","Epoch 1 Batch 100 Loss 8.8968 Accuracy 0.0153\n","Epoch 1 Batch 150 Loss 8.8009 Accuracy 0.0188\n","Epoch 1 Batch 200 Loss 8.6829 Accuracy 0.0205\n","Epoch 1 Batch 250 Loss 8.5476 Accuracy 0.0216\n","Epoch 1 Batch 300 Loss 8.4026 Accuracy 0.0223\n","Epoch 1 Batch 350 Loss 8.2562 Accuracy 0.0228\n","Epoch 1 Batch 400 Loss 8.1218 Accuracy 0.0232\n","Epoch 1 Batch 450 Loss 8.0035 Accuracy 0.0241\n","Epoch 1 Batch 500 Loss 7.8973 Accuracy 0.0258\n","Epoch 1 Batch 550 Loss 7.8018 Accuracy 0.0276\n","Epoch 1 Batch 600 Loss 7.7155 Accuracy 0.0294\n","Epoch 1 Batch 650 Loss 7.6369 Accuracy 0.0312\n","Epoch 1 Batch 700 Loss 7.5624 Accuracy 0.0330\n","Epoch 1 Batch 750 Loss 7.4937 Accuracy 0.0347\n","Epoch 1 Loss 7.4521 Accuracy 0.0357\n","Time taken for 1 epoch: 269.2932095527649 secs\n","\n","Epoch 2 Batch 0 Loss 6.2441 Accuracy 0.0625\n","Epoch 2 Batch 50 Loss 6.2757 Accuracy 0.0648\n","Epoch 2 Batch 100 Loss 6.2306 Accuracy 0.0660\n","Epoch 2 Batch 150 Loss 6.1772 Accuracy 0.0677\n","Epoch 2 Batch 200 Loss 6.1317 Accuracy 0.0693\n","Epoch 2 Batch 250 Loss 6.1014 Accuracy 0.0706\n","Epoch 2 Batch 300 Loss 6.0664 Accuracy 0.0720\n","Epoch 2 Batch 350 Loss 6.0307 Accuracy 0.0735\n","Epoch 2 Batch 400 Loss 5.9900 Accuracy 0.0749\n","Epoch 2 Batch 450 Loss 5.9490 Accuracy 0.0766\n","Epoch 2 Batch 500 Loss 5.9070 Accuracy 0.0783\n","Epoch 2 Batch 550 Loss 5.8588 Accuracy 0.0803\n","Epoch 2 Batch 600 Loss 5.8050 Accuracy 0.0828\n","Epoch 2 Batch 650 Loss 5.7446 Accuracy 0.0855\n","Epoch 2 Batch 700 Loss 5.6796 Accuracy 0.0885\n","Epoch 2 Batch 750 Loss 5.6084 Accuracy 0.0920\n","Epoch 2 Loss 5.5624 Accuracy 0.0943\n","Time taken for 1 epoch: 53.2454559803009 secs\n","\n","Epoch 3 Batch 0 Loss 4.1963 Accuracy 0.1803\n","Epoch 3 Batch 50 Loss 4.1371 Accuracy 0.1697\n","Epoch 3 Batch 100 Loss 4.0609 Accuracy 0.1741\n","Epoch 3 Batch 150 Loss 3.9705 Accuracy 0.1783\n","Epoch 3 Batch 200 Loss 3.8791 Accuracy 0.1835\n","Epoch 3 Batch 250 Loss 3.8066 Accuracy 0.1891\n","Epoch 3 Batch 300 Loss 3.7182 Accuracy 0.1946\n","Epoch 3 Batch 350 Loss 3.6376 Accuracy 0.1992\n","Epoch 3 Batch 400 Loss 3.5618 Accuracy 0.2038\n","Epoch 3 Batch 450 Loss 3.4785 Accuracy 0.2091\n","Epoch 3 Batch 500 Loss 3.4067 Accuracy 0.2137\n","Epoch 3 Batch 550 Loss 3.3315 Accuracy 0.2183\n","Epoch 3 Batch 600 Loss 3.2608 Accuracy 0.2227\n","Epoch 3 Batch 650 Loss 3.1877 Accuracy 0.2269\n","Epoch 3 Batch 700 Loss 3.1241 Accuracy 0.2305\n","Epoch 3 Batch 750 Loss 3.0679 Accuracy 0.2338\n","Epoch 3 Loss 3.0308 Accuracy 0.2361\n","Time taken for 1 epoch: 46.669055223464966 secs\n","\n","Epoch 4 Batch 0 Loss 2.1273 Accuracy 0.3041\n","Epoch 4 Batch 50 Loss 1.9192 Accuracy 0.2944\n","Epoch 4 Batch 100 Loss 1.8977 Accuracy 0.2966\n","Epoch 4 Batch 150 Loss 1.8806 Accuracy 0.2996\n","Epoch 4 Batch 200 Loss 1.8637 Accuracy 0.3011\n","Epoch 4 Batch 250 Loss 1.8486 Accuracy 0.3020\n","Epoch 4 Batch 300 Loss 1.8322 Accuracy 0.3033\n","Epoch 4 Batch 350 Loss 1.8206 Accuracy 0.3042\n","Epoch 4 Batch 400 Loss 1.8075 Accuracy 0.3046\n","Epoch 4 Batch 450 Loss 1.7805 Accuracy 0.3063\n","Epoch 4 Batch 500 Loss 1.7573 Accuracy 0.3075\n","Epoch 4 Batch 550 Loss 1.7370 Accuracy 0.3089\n","Epoch 4 Batch 600 Loss 1.7156 Accuracy 0.3103\n","Epoch 4 Batch 650 Loss 1.6947 Accuracy 0.3114\n","Epoch 4 Batch 700 Loss 1.6797 Accuracy 0.3120\n","Epoch 4 Batch 750 Loss 1.6642 Accuracy 0.3130\n","Epoch 4 Loss 1.6530 Accuracy 0.3134\n","Time taken for 1 epoch: 47.04292607307434 secs\n","\n","Epoch 5 Batch 0 Loss 1.3232 Accuracy 0.3951\n","Epoch 5 Batch 50 Loss 1.2087 Accuracy 0.3364\n","Epoch 5 Batch 100 Loss 1.2207 Accuracy 0.3347\n","Epoch 5 Batch 150 Loss 1.2180 Accuracy 0.3350\n","Epoch 5 Batch 200 Loss 1.2225 Accuracy 0.3363\n","Epoch 5 Batch 250 Loss 1.2219 Accuracy 0.3381\n","Epoch 5 Batch 300 Loss 1.2208 Accuracy 0.3382\n","Epoch 5 Batch 350 Loss 1.2164 Accuracy 0.3387\n","Epoch 5 Batch 400 Loss 1.2124 Accuracy 0.3376\n","Epoch 5 Batch 450 Loss 1.2095 Accuracy 0.3379\n","Epoch 5 Batch 500 Loss 1.2067 Accuracy 0.3379\n","Epoch 5 Batch 550 Loss 1.1964 Accuracy 0.3382\n","Epoch 5 Batch 600 Loss 1.1911 Accuracy 0.3385\n","Epoch 5 Batch 650 Loss 1.1850 Accuracy 0.3391\n","Epoch 5 Batch 700 Loss 1.1769 Accuracy 0.3404\n","Epoch 5 Batch 750 Loss 1.1714 Accuracy 0.3409\n","Saving checkpoint for epoch 5 at /content/drive/MyDrive/NLP_Task/CorrectSpellingTask//checkpoints/train_500k/ckpt-1\n","Epoch 5 Loss 1.1662 Accuracy 0.3413\n","Time taken for 1 epoch: 57.33702373504639 secs\n","\n","Epoch 6 Batch 0 Loss 0.8028 Accuracy 0.3454\n","Epoch 6 Batch 50 Loss 0.9960 Accuracy 0.3531\n","Epoch 6 Batch 100 Loss 0.9608 Accuracy 0.3529\n","Epoch 6 Batch 150 Loss 0.9569 Accuracy 0.3533\n","Epoch 6 Batch 200 Loss 0.9563 Accuracy 0.3550\n","Epoch 6 Batch 250 Loss 0.9494 Accuracy 0.3563\n","Epoch 6 Batch 300 Loss 0.9447 Accuracy 0.3569\n","Epoch 6 Batch 350 Loss 0.9404 Accuracy 0.3558\n","Epoch 6 Batch 400 Loss 0.9438 Accuracy 0.3548\n","Epoch 6 Batch 450 Loss 0.9412 Accuracy 0.3548\n","Epoch 6 Batch 500 Loss 0.9354 Accuracy 0.3559\n","Epoch 6 Batch 550 Loss 0.9308 Accuracy 0.3563\n","Epoch 6 Batch 600 Loss 0.9264 Accuracy 0.3565\n","Epoch 6 Batch 650 Loss 0.9224 Accuracy 0.3567\n","Epoch 6 Batch 700 Loss 0.9162 Accuracy 0.3576\n","Epoch 6 Batch 750 Loss 0.9124 Accuracy 0.3582\n","Epoch 6 Loss 0.9099 Accuracy 0.3588\n","Time taken for 1 epoch: 48.78761267662048 secs\n","\n","Epoch 7 Batch 0 Loss 0.8965 Accuracy 0.4058\n","Epoch 7 Batch 50 Loss 0.7560 Accuracy 0.3703\n","Epoch 7 Batch 100 Loss 0.7345 Accuracy 0.3741\n","Epoch 7 Batch 150 Loss 0.7328 Accuracy 0.3729\n","Epoch 7 Batch 200 Loss 0.7353 Accuracy 0.3723\n","Epoch 7 Batch 250 Loss 0.7384 Accuracy 0.3729\n","Epoch 7 Batch 300 Loss 0.7392 Accuracy 0.3725\n","Epoch 7 Batch 350 Loss 0.7406 Accuracy 0.3718\n","Epoch 7 Batch 400 Loss 0.7400 Accuracy 0.3723\n","Epoch 7 Batch 450 Loss 0.7404 Accuracy 0.3726\n","Epoch 7 Batch 500 Loss 0.7368 Accuracy 0.3728\n","Epoch 7 Batch 550 Loss 0.7364 Accuracy 0.3731\n","Epoch 7 Batch 600 Loss 0.7345 Accuracy 0.3736\n","Epoch 7 Batch 650 Loss 0.7293 Accuracy 0.3738\n","Epoch 7 Batch 700 Loss 0.7270 Accuracy 0.3736\n","Epoch 7 Batch 750 Loss 0.7243 Accuracy 0.3742\n","Epoch 7 Loss 0.7230 Accuracy 0.3742\n","Time taken for 1 epoch: 47.487059593200684 secs\n","\n","Epoch 8 Batch 0 Loss 0.6263 Accuracy 0.4107\n","Epoch 8 Batch 50 Loss 0.5898 Accuracy 0.3858\n","Epoch 8 Batch 100 Loss 0.5972 Accuracy 0.3880\n","Epoch 8 Batch 150 Loss 0.5947 Accuracy 0.3858\n","Epoch 8 Batch 200 Loss 0.5989 Accuracy 0.3875\n","Epoch 8 Batch 250 Loss 0.6032 Accuracy 0.3848\n","Epoch 8 Batch 300 Loss 0.6111 Accuracy 0.3852\n","Epoch 8 Batch 350 Loss 0.6122 Accuracy 0.3846\n","Epoch 8 Batch 400 Loss 0.6142 Accuracy 0.3833\n","Epoch 8 Batch 450 Loss 0.6160 Accuracy 0.3828\n","Epoch 8 Batch 500 Loss 0.6156 Accuracy 0.3823\n","Epoch 8 Batch 550 Loss 0.6138 Accuracy 0.3833\n","Epoch 8 Batch 600 Loss 0.6140 Accuracy 0.3836\n","Epoch 8 Batch 650 Loss 0.6127 Accuracy 0.3835\n","Epoch 8 Batch 700 Loss 0.6110 Accuracy 0.3839\n","Epoch 8 Batch 750 Loss 0.6091 Accuracy 0.3845\n","Epoch 8 Loss 0.6077 Accuracy 0.3846\n","Time taken for 1 epoch: 47.60984230041504 secs\n","\n","Epoch 9 Batch 0 Loss 0.6264 Accuracy 0.3397\n","Epoch 9 Batch 50 Loss 0.5102 Accuracy 0.3928\n","Epoch 9 Batch 100 Loss 0.5168 Accuracy 0.3905\n","Epoch 9 Batch 150 Loss 0.5151 Accuracy 0.3903\n","Epoch 9 Batch 200 Loss 0.5167 Accuracy 0.3896\n","Epoch 9 Batch 250 Loss 0.5218 Accuracy 0.3890\n","Epoch 9 Batch 300 Loss 0.5260 Accuracy 0.3898\n","Epoch 9 Batch 350 Loss 0.5258 Accuracy 0.3904\n","Epoch 9 Batch 400 Loss 0.5271 Accuracy 0.3904\n","Epoch 9 Batch 450 Loss 0.5295 Accuracy 0.3913\n","Epoch 9 Batch 500 Loss 0.5313 Accuracy 0.3917\n","Epoch 9 Batch 550 Loss 0.5322 Accuracy 0.3919\n","Epoch 9 Batch 600 Loss 0.5326 Accuracy 0.3920\n","Epoch 9 Batch 650 Loss 0.5311 Accuracy 0.3919\n","Epoch 9 Batch 700 Loss 0.5287 Accuracy 0.3925\n","Epoch 9 Batch 750 Loss 0.5282 Accuracy 0.3928\n","Epoch 9 Loss 0.5285 Accuracy 0.3924\n","Time taken for 1 epoch: 47.159650802612305 secs\n","\n","Epoch 10 Batch 0 Loss 0.4790 Accuracy 0.4075\n","Epoch 10 Batch 50 Loss 0.4792 Accuracy 0.3964\n","Epoch 10 Batch 100 Loss 0.4559 Accuracy 0.4010\n","Epoch 10 Batch 150 Loss 0.4499 Accuracy 0.3979\n","Epoch 10 Batch 200 Loss 0.4516 Accuracy 0.3979\n","Epoch 10 Batch 250 Loss 0.4514 Accuracy 0.3989\n","Epoch 10 Batch 300 Loss 0.4548 Accuracy 0.3990\n","Epoch 10 Batch 350 Loss 0.4599 Accuracy 0.3979\n","Epoch 10 Batch 400 Loss 0.4623 Accuracy 0.3978\n","Epoch 10 Batch 450 Loss 0.4666 Accuracy 0.3968\n","Epoch 10 Batch 500 Loss 0.4666 Accuracy 0.3971\n","Epoch 10 Batch 550 Loss 0.4671 Accuracy 0.3967\n","Epoch 10 Batch 600 Loss 0.4675 Accuracy 0.3970\n","Epoch 10 Batch 650 Loss 0.4672 Accuracy 0.3977\n","Epoch 10 Batch 700 Loss 0.4663 Accuracy 0.3984\n","Epoch 10 Batch 750 Loss 0.4652 Accuracy 0.3982\n","Saving checkpoint for epoch 10 at /content/drive/MyDrive/NLP_Task/CorrectSpellingTask//checkpoints/train_500k/ckpt-2\n","Epoch 10 Loss 0.4651 Accuracy 0.3981\n","Time taken for 1 epoch: 48.13109016418457 secs\n","\n","Epoch 11 Batch 0 Loss 0.2961 Accuracy 0.4412\n","Epoch 11 Batch 50 Loss 0.3890 Accuracy 0.4107\n","Epoch 11 Batch 100 Loss 0.3914 Accuracy 0.4057\n","Epoch 11 Batch 150 Loss 0.3939 Accuracy 0.4056\n","Epoch 11 Batch 200 Loss 0.3980 Accuracy 0.4047\n","Epoch 11 Batch 250 Loss 0.4038 Accuracy 0.4036\n","Epoch 11 Batch 300 Loss 0.4035 Accuracy 0.4031\n","Epoch 11 Batch 350 Loss 0.4068 Accuracy 0.4037\n","Epoch 11 Batch 400 Loss 0.4075 Accuracy 0.4037\n","Epoch 11 Batch 450 Loss 0.4095 Accuracy 0.4030\n","Epoch 11 Batch 500 Loss 0.4098 Accuracy 0.4032\n","Epoch 11 Batch 550 Loss 0.4106 Accuracy 0.4032\n","Epoch 11 Batch 600 Loss 0.4122 Accuracy 0.4025\n","Epoch 11 Batch 650 Loss 0.4140 Accuracy 0.4026\n","Epoch 11 Batch 700 Loss 0.4146 Accuracy 0.4025\n","Epoch 11 Batch 750 Loss 0.4152 Accuracy 0.4028\n","Epoch 11 Loss 0.4158 Accuracy 0.4031\n","Time taken for 1 epoch: 47.21628260612488 secs\n","\n","Epoch 12 Batch 0 Loss 0.4101 Accuracy 0.4563\n","Epoch 12 Batch 50 Loss 0.3538 Accuracy 0.4063\n","Epoch 12 Batch 100 Loss 0.3489 Accuracy 0.4057\n","Epoch 12 Batch 150 Loss 0.3529 Accuracy 0.4077\n","Epoch 12 Batch 200 Loss 0.3553 Accuracy 0.4073\n","Epoch 12 Batch 250 Loss 0.3600 Accuracy 0.4071\n","Epoch 12 Batch 300 Loss 0.3636 Accuracy 0.4058\n","Epoch 12 Batch 350 Loss 0.3680 Accuracy 0.4067\n","Epoch 12 Batch 400 Loss 0.3690 Accuracy 0.4065\n","Epoch 12 Batch 450 Loss 0.3705 Accuracy 0.4063\n","Epoch 12 Batch 500 Loss 0.3722 Accuracy 0.4062\n","Epoch 12 Batch 550 Loss 0.3731 Accuracy 0.4063\n","Epoch 12 Batch 600 Loss 0.3742 Accuracy 0.4072\n","Epoch 12 Batch 650 Loss 0.3743 Accuracy 0.4070\n","Epoch 12 Batch 700 Loss 0.3763 Accuracy 0.4069\n","Epoch 12 Batch 750 Loss 0.3773 Accuracy 0.4069\n","Epoch 12 Loss 0.3768 Accuracy 0.4072\n","Time taken for 1 epoch: 47.53221416473389 secs\n","\n","Epoch 13 Batch 0 Loss 0.2949 Accuracy 0.4215\n","Epoch 13 Batch 50 Loss 0.3193 Accuracy 0.4233\n","Epoch 13 Batch 100 Loss 0.3144 Accuracy 0.4196\n","Epoch 13 Batch 150 Loss 0.3192 Accuracy 0.4157\n","Epoch 13 Batch 200 Loss 0.3237 Accuracy 0.4144\n","Epoch 13 Batch 250 Loss 0.3272 Accuracy 0.4135\n","Epoch 13 Batch 300 Loss 0.3298 Accuracy 0.4119\n","Epoch 13 Batch 350 Loss 0.3316 Accuracy 0.4115\n","Epoch 13 Batch 400 Loss 0.3335 Accuracy 0.4105\n","Epoch 13 Batch 450 Loss 0.3362 Accuracy 0.4099\n","Epoch 13 Batch 500 Loss 0.3358 Accuracy 0.4104\n","Epoch 13 Batch 550 Loss 0.3359 Accuracy 0.4113\n","Epoch 13 Batch 600 Loss 0.3376 Accuracy 0.4114\n","Epoch 13 Batch 650 Loss 0.3389 Accuracy 0.4118\n","Epoch 13 Batch 700 Loss 0.3393 Accuracy 0.4117\n","Epoch 13 Batch 750 Loss 0.3404 Accuracy 0.4111\n","Epoch 13 Loss 0.3404 Accuracy 0.4110\n","Time taken for 1 epoch: 47.79163908958435 secs\n","\n","Epoch 14 Batch 0 Loss 0.3309 Accuracy 0.3926\n","Epoch 14 Batch 50 Loss 0.2902 Accuracy 0.4037\n","Epoch 14 Batch 100 Loss 0.2961 Accuracy 0.4097\n","Epoch 14 Batch 150 Loss 0.2957 Accuracy 0.4124\n","Epoch 14 Batch 200 Loss 0.2956 Accuracy 0.4145\n","Epoch 14 Batch 250 Loss 0.2995 Accuracy 0.4154\n","Epoch 14 Batch 300 Loss 0.2998 Accuracy 0.4158\n","Epoch 14 Batch 350 Loss 0.3020 Accuracy 0.4145\n","Epoch 14 Batch 400 Loss 0.3045 Accuracy 0.4129\n","Epoch 14 Batch 450 Loss 0.3075 Accuracy 0.4117\n","Epoch 14 Batch 500 Loss 0.3079 Accuracy 0.4123\n","Epoch 14 Batch 550 Loss 0.3082 Accuracy 0.4136\n","Epoch 14 Batch 600 Loss 0.3084 Accuracy 0.4139\n","Epoch 14 Batch 650 Loss 0.3091 Accuracy 0.4140\n","Epoch 14 Batch 700 Loss 0.3093 Accuracy 0.4143\n","Epoch 14 Batch 750 Loss 0.3111 Accuracy 0.4138\n","Epoch 14 Loss 0.3120 Accuracy 0.4141\n","Time taken for 1 epoch: 47.05088400840759 secs\n","\n","Epoch 15 Batch 0 Loss 0.1773 Accuracy 0.4768\n","Epoch 15 Batch 50 Loss 0.2637 Accuracy 0.4233\n","Epoch 15 Batch 100 Loss 0.2568 Accuracy 0.4174\n","Epoch 15 Batch 150 Loss 0.2587 Accuracy 0.4170\n","Epoch 15 Batch 200 Loss 0.2643 Accuracy 0.4149\n","Epoch 15 Batch 250 Loss 0.2674 Accuracy 0.4149\n","Epoch 15 Batch 300 Loss 0.2682 Accuracy 0.4162\n","Epoch 15 Batch 350 Loss 0.2711 Accuracy 0.4153\n","Epoch 15 Batch 400 Loss 0.2742 Accuracy 0.4159\n","Epoch 15 Batch 450 Loss 0.2768 Accuracy 0.4164\n","Epoch 15 Batch 500 Loss 0.2794 Accuracy 0.4166\n","Epoch 15 Batch 550 Loss 0.2808 Accuracy 0.4164\n","Epoch 15 Batch 600 Loss 0.2826 Accuracy 0.4162\n","Epoch 15 Batch 650 Loss 0.2831 Accuracy 0.4162\n","Epoch 15 Batch 700 Loss 0.2836 Accuracy 0.4162\n","Epoch 15 Batch 750 Loss 0.2842 Accuracy 0.4174\n","Saving checkpoint for epoch 15 at /content/drive/MyDrive/NLP_Task/CorrectSpellingTask//checkpoints/train_500k/ckpt-3\n","Epoch 15 Loss 0.2848 Accuracy 0.4171\n","Time taken for 1 epoch: 47.38558220863342 secs\n","\n","Epoch 16 Batch 0 Loss 0.2010 Accuracy 0.4279\n","Epoch 16 Batch 50 Loss 0.2448 Accuracy 0.4302\n","Epoch 16 Batch 100 Loss 0.2388 Accuracy 0.4269\n","Epoch 16 Batch 150 Loss 0.2430 Accuracy 0.4254\n","Epoch 16 Batch 200 Loss 0.2436 Accuracy 0.4244\n","Epoch 16 Batch 250 Loss 0.2451 Accuracy 0.4215\n","Epoch 16 Batch 300 Loss 0.2488 Accuracy 0.4206\n","Epoch 16 Batch 350 Loss 0.2536 Accuracy 0.4203\n","Epoch 16 Batch 400 Loss 0.2539 Accuracy 0.4208\n","Epoch 16 Batch 450 Loss 0.2560 Accuracy 0.4204\n","Epoch 16 Batch 500 Loss 0.2573 Accuracy 0.4203\n","Epoch 16 Batch 550 Loss 0.2589 Accuracy 0.4201\n","Epoch 16 Batch 600 Loss 0.2607 Accuracy 0.4203\n","Epoch 16 Batch 650 Loss 0.2607 Accuracy 0.4200\n","Epoch 16 Batch 700 Loss 0.2620 Accuracy 0.4199\n","Epoch 16 Batch 750 Loss 0.2637 Accuracy 0.4194\n","Epoch 16 Loss 0.2643 Accuracy 0.4191\n","Time taken for 1 epoch: 47.63494420051575 secs\n","\n","Epoch 17 Batch 0 Loss 0.2276 Accuracy 0.4163\n","Epoch 17 Batch 50 Loss 0.2163 Accuracy 0.4252\n","Epoch 17 Batch 100 Loss 0.2147 Accuracy 0.4244\n","Epoch 17 Batch 150 Loss 0.2181 Accuracy 0.4245\n","Epoch 17 Batch 200 Loss 0.2227 Accuracy 0.4252\n","Epoch 17 Batch 250 Loss 0.2270 Accuracy 0.4250\n","Epoch 17 Batch 300 Loss 0.2298 Accuracy 0.4240\n","Epoch 17 Batch 350 Loss 0.2320 Accuracy 0.4233\n","Epoch 17 Batch 400 Loss 0.2336 Accuracy 0.4233\n","Epoch 17 Batch 450 Loss 0.2350 Accuracy 0.4232\n","Epoch 17 Batch 500 Loss 0.2362 Accuracy 0.4229\n","Epoch 17 Batch 550 Loss 0.2369 Accuracy 0.4229\n","Epoch 17 Batch 600 Loss 0.2380 Accuracy 0.4228\n","Epoch 17 Batch 650 Loss 0.2387 Accuracy 0.4226\n","Epoch 17 Batch 700 Loss 0.2392 Accuracy 0.4220\n","Epoch 17 Batch 750 Loss 0.2398 Accuracy 0.4216\n","Epoch 17 Loss 0.2399 Accuracy 0.4218\n","Time taken for 1 epoch: 47.06090593338013 secs\n","\n","Epoch 18 Batch 0 Loss 0.1927 Accuracy 0.4601\n","Epoch 18 Batch 50 Loss 0.2091 Accuracy 0.4270\n","Epoch 18 Batch 100 Loss 0.2041 Accuracy 0.4257\n","Epoch 18 Batch 150 Loss 0.2069 Accuracy 0.4251\n","Epoch 18 Batch 200 Loss 0.2111 Accuracy 0.4245\n","Epoch 18 Batch 250 Loss 0.2109 Accuracy 0.4256\n","Epoch 18 Batch 300 Loss 0.2116 Accuracy 0.4243\n","Epoch 18 Batch 350 Loss 0.2136 Accuracy 0.4252\n","Epoch 18 Batch 400 Loss 0.2164 Accuracy 0.4237\n","Epoch 18 Batch 450 Loss 0.2181 Accuracy 0.4238\n","Epoch 18 Batch 500 Loss 0.2193 Accuracy 0.4239\n","Epoch 18 Batch 550 Loss 0.2202 Accuracy 0.4237\n","Epoch 18 Batch 600 Loss 0.2205 Accuracy 0.4237\n","Epoch 18 Batch 650 Loss 0.2210 Accuracy 0.4233\n","Epoch 18 Batch 700 Loss 0.2220 Accuracy 0.4232\n","Epoch 18 Batch 750 Loss 0.2228 Accuracy 0.4232\n","Epoch 18 Loss 0.2231 Accuracy 0.4232\n","Time taken for 1 epoch: 46.94210124015808 secs\n","\n","Epoch 19 Batch 0 Loss 0.1273 Accuracy 0.4502\n","Epoch 19 Batch 50 Loss 0.1861 Accuracy 0.4269\n","Epoch 19 Batch 100 Loss 0.1868 Accuracy 0.4256\n","Epoch 19 Batch 150 Loss 0.1878 Accuracy 0.4246\n","Epoch 19 Batch 200 Loss 0.1921 Accuracy 0.4255\n","Epoch 19 Batch 250 Loss 0.1952 Accuracy 0.4259\n","Epoch 19 Batch 300 Loss 0.1972 Accuracy 0.4275\n","Epoch 19 Batch 350 Loss 0.1996 Accuracy 0.4266\n","Epoch 19 Batch 400 Loss 0.2008 Accuracy 0.4253\n","Epoch 19 Batch 450 Loss 0.2011 Accuracy 0.4250\n","Epoch 19 Batch 500 Loss 0.2018 Accuracy 0.4254\n","Epoch 19 Batch 550 Loss 0.2030 Accuracy 0.4252\n","Epoch 19 Batch 600 Loss 0.2041 Accuracy 0.4249\n","Epoch 19 Batch 650 Loss 0.2048 Accuracy 0.4254\n","Epoch 19 Batch 700 Loss 0.2055 Accuracy 0.4253\n","Epoch 19 Batch 750 Loss 0.2068 Accuracy 0.4252\n","Epoch 19 Loss 0.2068 Accuracy 0.4255\n","Time taken for 1 epoch: 47.02378869056702 secs\n","\n","Epoch 20 Batch 0 Loss 0.1550 Accuracy 0.4191\n","Epoch 20 Batch 50 Loss 0.1756 Accuracy 0.4254\n","Epoch 20 Batch 100 Loss 0.1712 Accuracy 0.4238\n","Epoch 20 Batch 150 Loss 0.1745 Accuracy 0.4240\n","Epoch 20 Batch 200 Loss 0.1769 Accuracy 0.4247\n","Epoch 20 Batch 250 Loss 0.1793 Accuracy 0.4265\n","Epoch 20 Batch 300 Loss 0.1828 Accuracy 0.4259\n","Epoch 20 Batch 350 Loss 0.1842 Accuracy 0.4253\n","Epoch 20 Batch 400 Loss 0.1852 Accuracy 0.4245\n","Epoch 20 Batch 450 Loss 0.1867 Accuracy 0.4249\n","Epoch 20 Batch 500 Loss 0.1881 Accuracy 0.4259\n","Epoch 20 Batch 550 Loss 0.1893 Accuracy 0.4262\n","Epoch 20 Batch 600 Loss 0.1895 Accuracy 0.4263\n","Epoch 20 Batch 650 Loss 0.1913 Accuracy 0.4257\n","Epoch 20 Batch 700 Loss 0.1921 Accuracy 0.4263\n","Epoch 20 Batch 750 Loss 0.1932 Accuracy 0.4268\n","Saving checkpoint for epoch 20 at /content/drive/MyDrive/NLP_Task/CorrectSpellingTask//checkpoints/train_500k/ckpt-4\n","Epoch 20 Loss 0.1930 Accuracy 0.4272\n","Time taken for 1 epoch: 47.461559534072876 secs\n","\n"]}]},{"cell_type":"markdown","source":["**Evaluate**"],"metadata":{"id":"l5NRmEfbQebd"}},{"cell_type":"code","source":["MAX_LENGTH = 40\n","\n","def evaluate(inp_sentence):\n","  start_token = [tokenizer_ipt.vocab_size]\n","  end_token = [tokenizer_ipt.vocab_size + 1]\n","  \n","  # inp sentence is non_diacritic, hence adding the start and end token\n","  inp_sentence = start_token + tokenizer_ipt.encode(inp_sentence) + end_token\n","  encoder_input = tf.expand_dims(inp_sentence, 0)\n","  \n","  # as the target is exist diacritic, the first word to the transformer should be the\n","  # english start token.\n","  decoder_input = [tokenizer_opt.vocab_size]\n","  output = tf.expand_dims(decoder_input, 0)\n","    \n","  for i in range(MAX_LENGTH):\n","    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n","        encoder_input, output)\n","  \n","    # predictions.shape == (batch_size, seq_len, vocab_size)\n","    predictions, attention_weights = transformer(encoder_input, \n","                                                 output,\n","                                                 False,\n","                                                 enc_padding_mask,\n","                                                 combined_mask,\n","                                                 dec_padding_mask)\n","    \n","    # select the last word from the seq_len dimension\n","    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n","\n","    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","    \n","    # return the result if the predicted_id is equal to the end token\n","    if predicted_id == tokenizer_opt.vocab_size+1:\n","      return tf.squeeze(output, axis=0), attention_weights\n","    \n","    # concatentate the predicted_id to the output which is given to the decoder\n","    # as its input.\n","    output = tf.concat([output, predicted_id], axis=-1)\n","\n","  return tf.squeeze(output, axis=0), attention_weights"],"metadata":{"id":"JrjCtowEQkEq","executionInfo":{"status":"ok","timestamp":1655832569639,"user_tz":-420,"elapsed":405,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["def translate(sentence):\n","  result, attention_weights = evaluate(sentence)\n","  \n","  predicted_sentence = tokenizer_opt.decode([i for i in result \n","                                            if i < tokenizer_opt.vocab_size])  \n","\n","  print('Input: {}'.format(sentence))\n","  print('Predicted translation: {}'.format(predicted_sentence))"],"metadata":{"id":"rae2jv_jRERa","executionInfo":{"status":"ok","timestamp":1655832572809,"user_tz":-420,"elapsed":413,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["translate('toi la ai trong em')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RPHKrQ0sT_gg","executionInfo":{"status":"ok","timestamp":1655832654791,"user_tz":-420,"elapsed":1441,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}},"outputId":"eb669cee-5c02-4cbe-9545-84fb1a34d77f"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["Input: toi la ai trong em\n","Predicted translation: tôi là ai trong em\n"]}]},{"cell_type":"code","source":["translate('va hon toi tu do')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pQTLFC1LUUHC","executionInfo":{"status":"ok","timestamp":1655832668204,"user_tz":-420,"elapsed":1729,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}},"outputId":"2cb8f5c9-913d-415c-b5f4-9b9e154df1ee"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["Input: va hon toi tu do\n","Predicted translation: và hồn tôi từ đó\n"]}]},{"cell_type":"code","source":["translate('thich em hoi bi nhieu')"],"metadata":{"id":"W6b88lGGUnNH","executionInfo":{"status":"ok","timestamp":1655832747530,"user_tz":-420,"elapsed":1804,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}},"outputId":"e6c540ca-105b-4072-db26-aad6ff73a9a9","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["Input: thich em hoi bi nhieu\n","Predicted translation: thích em hội bị nhiều\n"]}]}]}