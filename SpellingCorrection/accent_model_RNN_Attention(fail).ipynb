{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"accent_model_RNN_Attention.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"kkhKDGbRPXfV"},"outputs":[],"source":["!pip install \"tensorflow-text==2.8.*\""]},{"cell_type":"code","source":["!pip install tensorflow_datasets"],"metadata":{"id":"yxHF1QnEV8Hl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import re\n","import itertools\n","import tqdm\n","import pickle\n","import random\n","import torch\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import tensorflow as tf\n","import tensorflow_text as tf_text\n","import unicodedata\n","import tensorflow_datasets\n","import typing\n","from typing import Any, Tuple\n","from tqdm import tqdm"],"metadata":{"id":"3RKHkps1QT_9","executionInfo":{"status":"ok","timestamp":1655698204982,"user_tz":-420,"elapsed":11315,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"IfFyjcaoQV5I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655698222742,"user_tz":-420,"elapsed":2894,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}},"outputId":"e997c95d-99b4-486c-bde0-32e7f6e0e6c1"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["**dataset**"],"metadata":{"id":"QsxDtMV-Qe1o"}},{"cell_type":"code","source":["data_path = \"/content/drive/MyDrive/NLP_Task/CorrectSpellingTask/train_tieng_viet.txt\"\n","with open(data_path) as f:\n","    train = f.readlines()"],"metadata":{"id":"YH7BWcE8RWGk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentences = []\n","for i in range(len(train)):\n","    [idx, sen] = train[i].split('\\t')\n","    sen = sen[:-1]\n","    sentences.append(sen)"],"metadata":{"id":"-lYsm6SsRedC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def normalizeString(s):\n","    # Tách dấu câu nếu kí tự liền nhau\n","    marks = '[.!?,-${}()]'\n","    r = \"([\"+\"\\\\\".join(marks)+\"])\"\n","    s = re.sub(r, r\" \\1 \", s)\n","    # Thay thế nhiều spaces bằng 1 space.\n","    s = re.sub(r\"\\s+\", r\" \", s).strip()\n","    return s\n","sentences = [normalizeString(sen).lower() for sen in sentences[:200000]]"],"metadata":{"id":"Y16hqbo6RgyM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _load_pickle(filename):\n","    pickle_in = open(filename,\"rb\")\n","    dict_ = pickle.load(pickle_in)\n","\n","    return dict_\n","\n","def _save_pickle(filename, obj):\n","  with open(filename, 'wb') as f:\n","    pickle.dump(obj, f)"],"metadata":{"id":"Svjf7z42RnwF","executionInfo":{"status":"ok","timestamp":1655698228570,"user_tz":-420,"elapsed":416,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["data_save = \"/content/drive/MyDrive/NLP_Task/CorrectSpellingTask/\"\n","\n","_save_pickle(data_save + 'sentences.pkl',sentences)"],"metadata":{"id":"0ggBkwD1RpmM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_save = \"/content/drive/MyDrive/NLP_Task/CorrectSpellingTask/\"\n","\n","sentences = _load_pickle(data_save + 'sentences.pkl')"],"metadata":{"id":"dfFeNpTsR_Na","executionInfo":{"status":"ok","timestamp":1655698230678,"user_tz":-420,"elapsed":543,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["**Split data**"],"metadata":{"id":"xTYw0YgoSIz4"}},{"cell_type":"code","source":["accented_chars = {\n","    'a': u'a á à ả ã ạ â ấ ầ ẩ ẫ ậ ă ắ ằ ẳ ẵ ặ',\n","    'o': u'o ó ò ỏ õ ọ ô ố ồ ổ ỗ ộ ơ ớ ờ ở ỡ ợ',\n","    'e': u'e é è ẻ ẽ ẹ ê ế ề ể ễ ệ',\n","    'u': u'u ú ù ủ ũ ụ ư ứ ừ ử ữ ự',\n","    'i': u'i í ì ỉ ĩ ị',\n","    'y': u'y ý ỳ ỷ ỹ ỵ',\n","    'd': u'd đ',\n","}\n","\n","plain_char_map = {}\n","for c, variants in accented_chars.items():\n","    for v in variants.split(' '):\n","        plain_char_map[v] = c\n","\n","\n","def remove_accent(text):\n","    return u''.join(plain_char_map.get(char, char) for char in text)"],"metadata":{"id":"MAKdxYcLTMJD","executionInfo":{"status":"ok","timestamp":1655698238964,"user_tz":-420,"elapsed":363,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","train_opt_160k = []\n","train_ipt_160k = []\n","val_opt_20k = []\n","val_ipt_20k = []\n","test_opt_20k = []\n","test_ipt_20k = []\n","\n","for i in tqdm(range(200000)):\n","    try:\n","      non_acc_seq = remove_accent(sentences[i])\n","    except:\n","      print('error remove tone line at sequence {}', str(i))\n","      next\n","    if i < 160000:\n","      train_opt_160k.append(sentences[i])\n","      train_ipt_160k.append(non_acc_seq)\n","    elif i < 180000:\n","      val_opt_20k.append(sentences[i])\n","      val_ipt_20k.append(non_acc_seq)\n","    else:\n","      test_opt_20k.append(sentences[i])\n","      test_ipt_20k.append(non_acc_seq)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fv3vp7b0T_Wr","executionInfo":{"status":"ok","timestamp":1655698251842,"user_tz":-420,"elapsed":11038,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}},"outputId":"c899c464-e8bd-4ed6-b5fc-d675203a15fa"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 200000/200000 [00:10<00:00, 18783.69it/s]\n"]}]},{"cell_type":"markdown","source":["**dataset**"],"metadata":{"id":"BX2SfxesUnG5"}},{"cell_type":"code","source":["train_examples = tf.data.Dataset.from_tensor_slices((train_ipt_160k, train_opt_160k))\n","val_examples = tf.data.Dataset.from_tensor_slices((val_ipt_20k, val_opt_20k))\n","test_examples = tf.data.Dataset.from_tensor_slices((test_ipt_20k, test_opt_20k))"],"metadata":{"id":"Xlm8pewmVIY2","executionInfo":{"status":"ok","timestamp":1655698266944,"user_tz":-420,"elapsed":10413,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# tokenizer_ipt = tensorflow_datasets.deprecated.text.SubwordTextEncoder.build_from_corpus(\n","#     (ipt.numpy() for (ipt, opt) in tqdm(train_examples)), target_vocab_size=500000)\n","\n","# tokenizer_opt = tensorflow_datasets.deprecated.text.SubwordTextEncoder.build_from_corpus(\n","#     (opt.numpy() for (ipt, opt) in tqdm(train_examples)), target_vocab_size=500000)"],"metadata":{"id":"UWp0QW3zXoJH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655606506633,"user_tz":-420,"elapsed":447711,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}},"outputId":"4d7dd450-c1e8-421d-fb58-df98e5bc0c19"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 160000/160000 [00:38<00:00, 4184.93it/s]\n","100%|██████████| 160000/160000 [00:40<00:00, 3964.94it/s]\n"]}]},{"cell_type":"code","source":["# data_save = \"/content/drive/MyDrive/NLP_Task/CorrectSpellingTask/\"\n","\n","# _save_pickle(data_save + 'tokenizer_ipt.pkl', tokenizer_ipt)\n","# _save_pickle(data_save + 'tokenizer_opt.pkl', tokenizer_opt)"],"metadata":{"id":"aKwsOkYzXoTD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data_save = \"/content/drive/MyDrive/NLP_Task/CorrectSpellingTask/\"\n","\n","# tokenizer_ipt = _load_pickle(data_save + 'tokenizer_ipt.pkl')\n","# tokenizer_opt = _load_pickle(data_save + 'tokenizer_opt.pkl')"],"metadata":{"id":"_PYa5fbZlCDz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**modify dataset**"],"metadata":{"id":"Z1TjzYvy5YE6"}},{"cell_type":"code","source":["def add_begin_end_sen(text):\n","    text = tf.strings.strip(text)\n","    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n","    return text"],"metadata":{"id":"XgPMdcyF5apN","executionInfo":{"status":"ok","timestamp":1655698271999,"user_tz":-420,"elapsed":3,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["max_vocab_size = 10000\n","\n","input_text_processor = tf.keras.layers.TextVectorization(\n","    standardize=add_begin_end_sen,\n","    max_tokens=max_vocab_size)\n","\n","input_text_processor.adapt(train_ipt_160k)"],"metadata":{"id":"KmiXVbAj6Rzj","executionInfo":{"status":"ok","timestamp":1655698300787,"user_tz":-420,"elapsed":27355,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["output_text_processor = tf.keras.layers.TextVectorization(\n","    standardize=add_begin_end_sen,\n","    max_tokens=max_vocab_size)\n","\n","output_text_processor.adapt(train_opt_160k)"],"metadata":{"id":"phHPVaXBBitq","executionInfo":{"status":"ok","timestamp":1655698325839,"user_tz":-420,"elapsed":15515,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["**example**"],"metadata":{"id":"SYCAQwT9dGlH"}},{"cell_type":"code","source":["#hyper-parameter\n","BUFFER_SIZE = 200000\n","BATCH_SIZE = 32\n","\n","#dataset for tensorflow\n","MAX_LENGTH = 30\n","\n","def filter_max_length(x, y, max_length=MAX_LENGTH):\n","  return tf.logical_and(tf.size(x) <= max_length,\n","                        tf.size(y) <= max_length)\n","train_dataset = train_examples.filter(filter_max_length)\n","train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","\n","val_dataset = val_examples.filter(filter_max_length)\n","val_dataset = val_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"],"metadata":{"id":"kteyp0vDdscy","executionInfo":{"status":"ok","timestamp":1655699025090,"user_tz":-420,"elapsed":390,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":["**EMBEDDING_DIM**"],"metadata":{"id":"atrQi4Wul1zn"}},{"cell_type":"code","source":["class ShapeChecker():\n","  def __init__(self):\n","    # Keep a cache of every axis-name seen\n","    self.shapes = {}\n","\n","  def __call__(self, tensor, names, broadcast=False):\n","    if not tf.executing_eagerly():\n","      return\n","\n","    if isinstance(names, str):\n","      names = (names,)\n","\n","    shape = tf.shape(tensor)\n","    rank = tf.rank(tensor)\n","\n","    if rank != len(names):\n","      raise ValueError(f'Rank mismatch:\\n'\n","                       f'    found {rank}: {shape.numpy()}\\n'\n","                       f'    expected {len(names)}: {names}\\n')\n","\n","    for i, name in enumerate(names):\n","      if isinstance(name, int):\n","        old_dim = name\n","      else:\n","        old_dim = self.shapes.get(name, None)\n","      new_dim = shape[i]\n","\n","      if (broadcast and new_dim == 1):\n","        continue\n","\n","      if old_dim is None:\n","        # If the axis name is new, add its length to the cache.\n","        self.shapes[name] = new_dim\n","        continue\n","\n","      if new_dim != old_dim:\n","        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n","                         f\"    found: {new_dim}\\n\"\n","                         f\"    expected: {old_dim}\\n\")\n","\n","embedding_dim = 128\n","units = 256\n","\n","class Encoder(tf.keras.layers.Layer):\n","  def __init__(self, input_vocab_size, embedding_dim, enc_units):\n","    super(Encoder, self).__init__()\n","    self.enc_units = enc_units\n","    self.input_vocab_size = input_vocab_size\n","\n","    # The embedding layer converts tokens to vectors\n","    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n","                                               embedding_dim)\n","\n","    # The GRU RNN layer processes those vectors sequentially.\n","    self.gru = tf.keras.layers.GRU(self.enc_units,\n","                                   # Return the sequence and state\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","\n","  def call(self, tokens, state=None):\n","    shape_checker = ShapeChecker()\n","    shape_checker(tokens, ('batch', 's'))\n","\n","    # 2. The embedding layer looks up the embedding for each token.\n","    vectors = self.embedding(tokens)\n","    shape_checker(vectors, ('batch', 's', 'embed_dim'))\n","\n","    # 3. The GRU processes the embedding sequence.\n","    #    output shape: (batch, s, enc_units)\n","    #    state shape: (batch, enc_units)\n","    output, state = self.gru(vectors, initial_state=state)\n","    shape_checker(output, ('batch', 's', 'enc_units'))\n","    shape_checker(state, ('batch', 'enc_units'))\n","\n","    # 4. Returns the new sequence and its state.\n","    return output, state"],"metadata":{"id":"MolZwW10l4JH","executionInfo":{"status":"ok","timestamp":1655698340306,"user_tz":-420,"elapsed":415,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["**TEST ENCODER**"],"metadata":{"id":"JqgrDEvErx-J"}},{"cell_type":"code","source":["example_input_batch, example_target_batch = next(iter(train_dataset))"],"metadata":{"id":"Skwx4WNEqCdI","executionInfo":{"status":"ok","timestamp":1655698346458,"user_tz":-420,"elapsed":2813,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# Convert the input text to tokens.\n","example_tokens = input_text_processor(example_input_batch)\n","\n","# Encode the input sequence.\n","encoder = Encoder(input_text_processor.vocabulary_size(),\n","                  embedding_dim, units)\n","example_enc_output, example_enc_state = encoder(example_tokens)\n","\n","print(f'Input batch, shape (batch): {example_input_batch.shape}')\n","print(f'Input batch tokens, shape (batch, s): {example_tokens.shape}')\n","print(f'Encoder output, shape (batch, s, units): {example_enc_output.shape}')\n","print(f'Encoder state, shape (batch, units): {example_enc_state.shape}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BdkILTt-p0rR","executionInfo":{"status":"ok","timestamp":1655698355787,"user_tz":-420,"elapsed":3702,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}},"outputId":"3eeb760d-3e75-471c-9ff1-a7ac16d91e21"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Input batch, shape (batch): (32,)\n","Input batch tokens, shape (batch, s): (32, 248)\n","Encoder output, shape (batch, s, units): (32, 248, 256)\n","Encoder state, shape (batch, units): (32, 256)\n"]}]},{"cell_type":"markdown","source":["**ANTTENTION_LAYER**"],"metadata":{"id":"dCSmYmztsQA_"}},{"cell_type":"code","source":["\n","class BahdanauAttention(tf.keras.layers.Layer):\n","  def __init__(self, units):\n","    super().__init__()\n","    # For Eqn. (4), the  Bahdanau attention\n","    self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n","    self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n","\n","    self.attention = tf.keras.layers.AdditiveAttention()\n","\n","  def call(self, query, value, mask):\n","    shape_checker = ShapeChecker()\n","    shape_checker(query, ('batch', 't', 'query_units'))\n","    shape_checker(value, ('batch', 's', 'value_units'))\n","    shape_checker(mask, ('batch', 's'))\n","\n","    # From Eqn. (4), `W1@ht`.\n","    w1_query = self.W1(query)\n","    shape_checker(w1_query, ('batch', 't', 'attn_units'))\n","\n","    # From Eqn. (4), `W2@hs`.\n","    w2_key = self.W2(value)\n","    shape_checker(w2_key, ('batch', 's', 'attn_units'))\n","\n","    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n","    value_mask = mask\n","\n","    context_vector, attention_weights = self.attention(\n","        inputs = [w1_query, value, w2_key],\n","        mask=[query_mask, value_mask],\n","        return_attention_scores = True,\n","    )\n","    shape_checker(context_vector, ('batch', 't', 'value_units'))\n","    shape_checker(attention_weights, ('batch', 't', 's'))\n","\n","    return context_vector, attention_weights"],"metadata":{"id":"Bmmdc9JfsUvK","executionInfo":{"status":"ok","timestamp":1655698361959,"user_tz":-420,"elapsed":395,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["attention_layer = BahdanauAttention(units)"],"metadata":{"id":"uro2r8zusmm3","executionInfo":{"status":"ok","timestamp":1655698364456,"user_tz":-420,"elapsed":395,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["**Test Attention Layer**"],"metadata":{"id":"cQGFrdZttJfv"}},{"cell_type":"code","source":["# Later, the decoder will generate this attention query\n","example_attention_query = tf.random.normal(shape=[len(example_input_batch), 2, 10])\n","\n","# Attend to the encoded tokens\n","\n","context_vector, attention_weights = attention_layer(\n","    query=example_attention_query,\n","    value=example_enc_output,\n","    mask=(example_tokens != 0))\n","\n","print(f'Attention result shape: (batch_size, query_seq_length, units):           {context_vector.shape}')\n","print(f'Attention weights shape: (batch_size, query_seq_length, value_seq_length): {attention_weights.shape}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mwn3enxYtLN6","executionInfo":{"status":"ok","timestamp":1655698370096,"user_tz":-420,"elapsed":4322,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}},"outputId":"4e72d7ca-7c5a-4432-9d57-b70e383fc224"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Attention result shape: (batch_size, query_seq_length, units):           (32, 2, 256)\n","Attention weights shape: (batch_size, query_seq_length, value_seq_length): (32, 2, 248)\n"]}]},{"cell_type":"code","source":["class DecoderInput(typing.NamedTuple):\n","    new_tokens: Any\n","    enc_output: Any\n","    mask: Any\n","\n","class DecoderOutput(typing.NamedTuple):\n","    logits: Any\n","    attention_weights: Any"],"metadata":{"id":"7lsgNkhQv_PY","executionInfo":{"status":"ok","timestamp":1655698373069,"user_tz":-420,"elapsed":547,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["**DECODER LAYER**"],"metadata":{"id":"qBoBRrk2t8I_"}},{"cell_type":"code","source":["class Decoder(tf.keras.layers.Layer):\n","  def __init__(self, output_vocab_size, embedding_dim, dec_units):\n","    super(Decoder, self).__init__()\n","    self.dec_units = dec_units\n","    self.output_vocab_size = output_vocab_size\n","    self.embedding_dim = embedding_dim\n","\n","    # For Step 1. The embedding layer convets token IDs to vectors\n","    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n","                                               embedding_dim)\n","\n","    # For Step 2. The RNN keeps track of what's been generated so far.\n","    self.gru = tf.keras.layers.GRU(self.dec_units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","\n","    # For step 3. The RNN output will be the query for the attention layer.\n","    self.attention = BahdanauAttention(self.dec_units)\n","\n","    # For step 4. Eqn. (3): converting `ct` to `at`\n","    self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n","                                    use_bias=False)\n","\n","    # For step 5. This fully connected layer produces the logits for each\n","    # output token.\n","    self.fc = tf.keras.layers.Dense(self.output_vocab_size)\n","  \n","  def call(self,\n","         inputs: DecoderInput,\n","         state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n","    shape_checker = ShapeChecker()\n","    shape_checker(inputs.new_tokens, ('batch', 't'))\n","    shape_checker(inputs.enc_output, ('batch', 's', 'enc_units'))\n","    shape_checker(inputs.mask, ('batch', 's'))\n","\n","    if state is not None:\n","      shape_checker(state, ('batch', 'dec_units'))\n","\n","    # Step 1. Lookup the embeddings\n","    vectors = self.embedding(inputs.new_tokens)\n","    shape_checker(vectors, ('batch', 't', 'embedding_dim'))\n","\n","    # Step 2. Process one step with the RNN\n","    rnn_output, state = self.gru(vectors, initial_state=state)\n","\n","    shape_checker(rnn_output, ('batch', 't', 'dec_units'))\n","    shape_checker(state, ('batch', 'dec_units'))\n","\n","    # Step 3. Use the RNN output as the query for the attention over the\n","    # encoder output.\n","    context_vector, attention_weights = self.attention(\n","        query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n","    shape_checker(context_vector, ('batch', 't', 'dec_units'))\n","    shape_checker(attention_weights, ('batch', 't', 's'))\n","\n","    # Step 4. Eqn. (3): Join the context_vector and rnn_output\n","    #     [ct; ht] shape: (batch t, value_units + query_units)\n","    context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n","\n","    # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n","    attention_vector = self.Wc(context_and_rnn_output)\n","    shape_checker(attention_vector, ('batch', 't', 'dec_units'))\n","\n","    # Step 5. Generate logit predictions:\n","    logits = self.fc(attention_vector)\n","    shape_checker(logits, ('batch', 't', 'output_vocab_size'))\n","\n","    return DecoderOutput(logits, attention_weights), state\n"],"metadata":{"id":"CJiaXCv8t_O7","executionInfo":{"status":"ok","timestamp":1655698500798,"user_tz":-420,"elapsed":4,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["decoder = Decoder(output_text_processor.vocabulary_size(),embedding_dim, units)"],"metadata":{"id":"IkVIuOKVuvVM","executionInfo":{"status":"ok","timestamp":1655698504425,"user_tz":-420,"elapsed":387,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["**Test Decoder**"],"metadata":{"id":"hktvnmilxw8T"}},{"cell_type":"code","source":["# Convert the target sequence, and collect the \"[START]\" tokens\n","example_output_tokens = output_text_processor(example_target_batch)\n","\n","start_index = output_text_processor.get_vocabulary().index('[START]')\n","first_token = tf.constant([[start_index]] * example_output_tokens.shape[0])\n","\n","dec_result, dec_state = decoder(\n","    inputs = DecoderInput(new_tokens=first_token,\n","                          enc_output=example_enc_output,\n","                          mask=(example_tokens != 0)),\n","    state = example_enc_state\n",")\n","\n","print(f'logits shape: (batch_size, t, output_vocab_size) {dec_result.logits.shape}')\n","print(f'state shape: (batch_size, dec_units) {dec_state.shape}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9QdeI3c_x8_0","executionInfo":{"status":"ok","timestamp":1655698505957,"user_tz":-420,"elapsed":5,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}},"outputId":"5d96517b-a002-40ad-c0fe-45bc8134c035"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["logits shape: (batch_size, t, output_vocab_size) (32, 1, 10000)\n","state shape: (batch_size, dec_units) (32, 256)\n"]}]},{"cell_type":"code","source":["sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)\n","vocab = np.array(output_text_processor.get_vocabulary())\n","first_word = vocab[sampled_token.numpy()]\n","first_word[:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r7pmXcztyw4y","executionInfo":{"status":"ok","timestamp":1655698509927,"user_tz":-420,"elapsed":433,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}},"outputId":"c2a927d8-35dd-42f6-d7b4-c41b953f0ef4"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([['đẹp'],\n","       ['đợi'],\n","       ['239'],\n","       ['\"\"trung'],\n","       ['uchiha']], dtype='<U18')"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["**TRAINING**"],"metadata":{"id":"ixDu2O3S11KM"}},{"cell_type":"code","source":["class MaskedLoss(tf.keras.losses.Loss):\n","  def __init__(self):\n","    self.name = 'masked_loss'\n","    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","        from_logits=True, reduction='none')\n","\n","  def __call__(self, y_true, y_pred):\n","    shape_checker = ShapeChecker()\n","    shape_checker(y_true, ('batch', 't'))\n","    shape_checker(y_pred, ('batch', 't', 'logits'))\n","\n","    # Calculate the loss for each item in the batch.\n","    loss = self.loss(y_true, y_pred)\n","    shape_checker(loss, ('batch', 't'))\n","\n","    # Mask off the losses on padding.\n","    mask = tf.cast(y_true != 0, tf.float32)\n","    shape_checker(mask, ('batch', 't'))\n","    loss *= mask\n","\n","    # Return the total.\n","    return tf.reduce_sum(loss)\n","\n","class TrainTranslator(tf.keras.Model):\n","  def __init__(self, embedding_dim, units,\n","               input_text_processor,\n","               output_text_processor, \n","               use_tf_function=True):\n","    super().__init__()\n","    # Build the encoder and decoder\n","    encoder = Encoder(input_text_processor.vocabulary_size(),\n","                      embedding_dim, units)\n","    decoder = Decoder(output_text_processor.vocabulary_size(),\n","                      embedding_dim, units)\n","\n","    self.encoder = encoder\n","    self.decoder = decoder\n","    self.input_text_processor = input_text_processor\n","    self.output_text_processor = output_text_processor\n","    self.use_tf_function = use_tf_function\n","    self.shape_checker = ShapeChecker()\n","\n","  def train_step(self, inputs):\n","    self.shape_checker = ShapeChecker()\n","    if self.use_tf_function:\n","      return self._tf_train_step(inputs)\n","    else:\n","      return self._train_step(inputs)\n","  \n","  def _preprocess(self, input_text, target_text):\n","    self.shape_checker(input_text, ('batch',))\n","    self.shape_checker(target_text, ('batch',))\n","\n","    # Convert the text to token IDs\n","    input_tokens = self.input_text_processor(input_text)\n","    target_tokens = self.output_text_processor(target_text)\n","    self.shape_checker(input_tokens, ('batch', 's'))\n","    self.shape_checker(target_tokens, ('batch', 't'))\n","\n","    # Convert IDs to masks.\n","    input_mask = input_tokens != 0\n","    self.shape_checker(input_mask, ('batch', 's'))\n","\n","    target_mask = target_tokens != 0\n","    self.shape_checker(target_mask, ('batch', 't'))\n","\n","    return input_tokens, input_mask, target_tokens, target_mask\n","\n","  def _train_step(self, inputs):\n","    input_text, target_text = inputs  \n","\n","    (input_tokens, input_mask,\n","    target_tokens, target_mask) = self._preprocess(input_text, target_text)\n","\n","    max_target_length = tf.shape(target_tokens)[1]\n","\n","    with tf.GradientTape() as tape:\n","      # Encode the input\n","      enc_output, enc_state = self.encoder(input_tokens)\n","      self.shape_checker(enc_output, ('batch', 's', 'enc_units'))\n","      self.shape_checker(enc_state, ('batch', 'enc_units'))\n","\n","      # Initialize the decoder's state to the encoder's final state.\n","      # This only works if the encoder and decoder have the same number of\n","      # units.\n","      dec_state = enc_state\n","      loss = tf.constant(0.0)\n","\n","      for t in tf.range(max_target_length-1):\n","        # Pass in two tokens from the target sequence:\n","        # 1. The current input to the decoder.\n","        # 2. The target for the decoder's next prediction.\n","        new_tokens = target_tokens[:, t:t+2]\n","        step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n","                                              enc_output, dec_state)\n","        loss = loss + step_loss\n","\n","      # Average the loss over all non padding tokens.\n","      average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n","\n","    # Apply an optimization step\n","    variables = self.trainable_variables \n","    gradients = tape.gradient(average_loss, variables)\n","    self.optimizer.apply_gradients(zip(gradients, variables))\n","\n","    # Return a dict mapping metric names to current value\n","    return {'batch_loss': average_loss}\n","\n","  def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n","    input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n","\n","    # Run the decoder one step.\n","    decoder_input = DecoderInput(new_tokens=input_token,\n","                                enc_output=enc_output,\n","                                mask=input_mask)\n","\n","    dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n","    self.shape_checker(dec_result.logits, ('batch', 't1', 'logits'))\n","    self.shape_checker(dec_result.attention_weights, ('batch', 't1', 's'))\n","    self.shape_checker(dec_state, ('batch', 'dec_units'))\n","\n","    # `self.loss` returns the total for non-padded tokens\n","    y = target_token\n","    y_pred = dec_result.logits\n","    step_loss = self.loss(y, y_pred)\n","\n","    return step_loss, dec_state"],"metadata":{"id":"l6QHzT1V12vU","executionInfo":{"status":"ok","timestamp":1655698560783,"user_tz":-420,"elapsed":390,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["translator = TrainTranslator(\n","    embedding_dim, units,\n","    input_text_processor=input_text_processor,\n","    output_text_processor=output_text_processor,\n","    use_tf_function=False)\n","\n","# Configure the loss and optimizer\n","translator.compile(\n","    optimizer=tf.optimizers.Adam(),\n","    loss=MaskedLoss(),\n",")"],"metadata":{"id":"B3ph9GH63UuG","executionInfo":{"status":"ok","timestamp":1655698816683,"user_tz":-420,"elapsed":532,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["for n in range(10):\n","  print(translator.train_step([example_input_batch, example_target_batch]))\n","print()"],"metadata":{"id":"j4bcDDUh30un","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655698663739,"user_tz":-420,"elapsed":94940,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}},"outputId":"500d8496-1aeb-42ba-c4f9-52852a86b347"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=9.092292>}\n","{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=9.088858>}\n","{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=9.084632>}\n","{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=9.078565>}\n","{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=9.068831>}\n","{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=9.052194>}\n","{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=9.022856>}\n","{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=8.970242>}\n","{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=8.87406>}\n","{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=8.69227>}\n","\n"]}]},{"cell_type":"code","source":["@tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n","                               tf.TensorSpec(dtype=tf.string, shape=[None])]])\n","def _tf_train_step(self, inputs):\n","  return self._train_step(inputs)\n","\n","TrainTranslator._tf_train_step = _tf_train_step\n","translator.use_tf_function = True"],"metadata":{"id":"kq8xWrsMVQaa","executionInfo":{"status":"ok","timestamp":1655698702590,"user_tz":-420,"elapsed":398,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["translator.train_step([example_input_batch, example_target_batch])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"103_uiX2VUab","executionInfo":{"status":"ok","timestamp":1655698721024,"user_tz":-420,"elapsed":9532,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}},"outputId":"9933e22d-09c4-49cf-d0fc-60ad9614d163"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=8.328201>}"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["%%time\n","for n in range(10):\n","  print(translator.train_step([example_input_batch, example_target_batch]))\n","print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4WDj-Sv5VcPv","executionInfo":{"status":"ok","timestamp":1655698775549,"user_tz":-420,"elapsed":37011,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}},"outputId":"a41efb8d-48de-4875-c3f9-2cc4b4f54f5e"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.5701733>}\n","{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=6.8491893>}\n","{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=6.5922775>}\n","{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=6.429686>}\n","{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=6.3043146>}\n","{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=6.208673>}\n","{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=6.1390076>}\n","{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=6.090383>}\n","{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=6.057222>}\n","{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=6.0347743>}\n","\n","CPU times: user 55.4 s, sys: 6.78 s, total: 1min 2s\n","Wall time: 36.7 s\n"]}]},{"cell_type":"markdown","source":["**Train the model**"],"metadata":{"id":"GYXoUjxTVyjj"}},{"cell_type":"code","source":["train_translator = TrainTranslator(\n","    embedding_dim, units,\n","    input_text_processor=input_text_processor,\n","    output_text_processor=output_text_processor)\n","\n","# Configure the loss and optimizer\n","train_translator.compile(\n","    optimizer=tf.optimizers.Adam(),\n","    loss=MaskedLoss(),\n",")"],"metadata":{"id":"gVEu41VmV9ZT","executionInfo":{"status":"ok","timestamp":1655699216772,"user_tz":-420,"elapsed":399,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["class BatchLogs(tf.keras.callbacks.Callback):\n","  def __init__(self, key):\n","    self.key = key\n","    self.logs = []\n","\n","  def on_train_batch_end(self, n, logs):\n","    self.logs.append(logs[self.key])\n","\n","batch_loss = BatchLogs('batch_loss')\n","train_translator.fit(train_dataset, epochs=3,steps_per_epoch=30,callbacks=[batch_loss])"],"metadata":{"id":"8FyRu57NWyum"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Get sentences**"],"metadata":{"id":"bf3_V42zYv4D"}},{"cell_type":"code","source":["class Translator(tf.Module):\n","  def __init__(self, encoder, decoder, input_text_processor,\n","               output_text_processor):\n","    self.encoder = encoder\n","    self.decoder = decoder\n","    self.input_text_processor = input_text_processor\n","    self.output_text_processor = output_text_processor\n","\n","    self.output_token_string_from_index = (\n","        tf.keras.layers.StringLookup(\n","            vocabulary=output_text_processor.get_vocabulary(),\n","            mask_token='',\n","            invert=True))\n","\n","    # The output should never generate padding, unknown, or start.\n","    index_from_string = tf.keras.layers.StringLookup(\n","        vocabulary=output_text_processor.get_vocabulary(), mask_token='')\n","    token_mask_ids = index_from_string(['', '[UNK]', '[START]']).numpy()\n","\n","    token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n","    token_mask[np.array(token_mask_ids)] = True\n","    self.token_mask = token_mask\n","\n","    self.start_token = index_from_string(tf.constant('[START]'))\n","    self.end_token = index_from_string(tf.constant('[END]'))\n","\n","  def tokens_to_text(self, result_tokens):\n","    shape_checker = ShapeChecker()\n","    shape_checker(result_tokens, ('batch', 't'))\n","    result_text_tokens = self.output_token_string_from_index(result_tokens)\n","    shape_checker(result_text_tokens, ('batch', 't'))\n","\n","    result_text = tf.strings.reduce_join(result_text_tokens,\n","                                        axis=1, separator=' ')\n","    shape_checker(result_text, ('batch'))\n","\n","    result_text = tf.strings.strip(result_text)\n","    shape_checker(result_text, ('batch',))\n","    return result_text\n","  \n","  def sample(self, logits, temperature):\n","    shape_checker = ShapeChecker()\n","    # 't' is usually 1 here.\n","    shape_checker(logits, ('batch', 't', 'vocab'))\n","    shape_checker(self.token_mask, ('vocab',))\n","\n","    token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n","    shape_checker(token_mask, ('batch', 't', 'vocab'), broadcast=True)\n","\n","    # Set the logits for all masked tokens to -inf, so they are never chosen.\n","    logits = tf.where(self.token_mask, -np.inf, logits)\n","\n","    if temperature == 0.0:\n","      new_tokens = tf.argmax(logits, axis=-1)\n","    else: \n","      logits = tf.squeeze(logits, axis=1)\n","      new_tokens = tf.random.categorical(logits/temperature,\n","                                          num_samples=1)\n","\n","    shape_checker(new_tokens, ('batch', 't'))\n","\n","    return new_tokens\n","\n","  def translate(self,\n","                       input_text, *,\n","                       max_length=50,\n","                       return_attention=True,\n","                       temperature=1.0):\n","    batch_size = tf.shape(input_text)[0]\n","    input_tokens = self.input_text_processor(input_text)\n","    enc_output, enc_state = self.encoder(input_tokens)\n","\n","    dec_state = enc_state\n","    new_tokens = tf.fill([batch_size, 1], self.start_token)\n","\n","    result_tokens = []\n","    attention = []\n","    done = tf.zeros([batch_size, 1], dtype=tf.bool)\n","\n","    for _ in range(max_length):\n","      dec_input = DecoderInput(new_tokens=new_tokens,\n","                              enc_output=enc_output,\n","                              mask=(input_tokens!=0))\n","\n","      dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n","\n","      attention.append(dec_result.attention_weights)\n","\n","      new_tokens = self.sample(dec_result.logits, temperature)\n","\n","      # If a sequence produces an `end_token`, set it `done`\n","      done = done | (new_tokens == self.end_token)\n","      # Once a sequence is done it only produces 0-padding.\n","      new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n","\n","      # Collect the generated tokens\n","      result_tokens.append(new_tokens)\n","\n","      if tf.executing_eagerly() and tf.reduce_all(done):\n","        break\n","\n","    # Convert the list of generates token ids to a list of strings.\n","    result_tokens = tf.concat(result_tokens, axis=-1)\n","    result_text = self.tokens_to_text(result_tokens)\n","\n","    if return_attention:\n","      attention_stack = tf.concat(attention, axis=1)\n","      return {'text': result_text, 'attention': attention_stack}\n","    else:\n","      return {'text': result_text}\n"],"metadata":{"id":"Q80RhQO2YyPf","executionInfo":{"status":"ok","timestamp":1655700764775,"user_tz":-420,"elapsed":3,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["translator = Translator(\n","    encoder=train_translator.encoder,\n","    decoder=train_translator.decoder,\n","    input_text_processor=input_text_processor,\n","    output_text_processor=output_text_processor,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z9V6CONUcFhl","executionInfo":{"status":"ok","timestamp":1655700768970,"user_tz":-420,"elapsed":412,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}},"outputId":"58100af8-1bc7-4b89-db7f-b999bee9041f"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"]}]},{"cell_type":"code","source":["input_text = tf.constant([\n","    'trinh va em', #\"trịnh và em\"\n","    'nen la em', # \"và hôn tôi từ đó\"\n","])\n","\n","result = translator.translate(\n","    input_text = input_text)\n","\n","print(result['text'][0].numpy().decode())\n","print(result['text'][1].numpy().decode())\n","print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lruujpDNcc-M","executionInfo":{"status":"ok","timestamp":1655700805416,"user_tz":-420,"elapsed":2744,"user":{"displayName":"Nguyễn Nathan","userId":"02063277889349077620"}},"outputId":"d94e2fda-da98-4d5c-bba3-d6f262450eb7"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["văn nhân sản địa từ của của tây trình nay vụ năm suất gián quân độ có\n","axit khi hợp phủ sự - , nước giáo trại ( dài mang lá vị lập\" chiến đức sự . có tiếng , , lần tiền thái chủ ra và tảng còn hành định - khác những 1 đối đạt công , với khối phủ đều harry chối phi gắn\n","\n"]}]}]}